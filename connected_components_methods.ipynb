{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "connected_components_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2HCCr8YlPcjy8R8JXWvXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brent-Morrison/PySpark_examples/blob/main/connected_components_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW9bfj8zsMuA"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook will document various methods for identifying connected components from a list of paired records.  This problem is best understood with an example.  \n",
        "  \n",
        "Consider a matching process identifying related records.  This is represented in the table below.  Record 1 is matched to record 2, and record 2 is matched to record 3.  These three make a group of connected components.  Records 4 and 5 also represent a group.\n",
        "<br>\n",
        "\n",
        "| record 1 \t| record 2 \t| match key \t|\n",
        "|----------\t|----------\t|-----------\t|\n",
        "| 1        \t| 2        \t| 1_2_3     \t|\n",
        "| 2        \t| 3        \t| 1_2_3     \t|\n",
        "| 4        \t| 5        \t| 4_5       \t|\n",
        "<br>\n",
        "\n",
        "Assigning a group label (```match key``` above) is simple when the pairs of records are each distinct over the full data set.\n",
        "\n",
        "Assigning a group label can be challenging however in situations where more than two records are matched.  \n",
        "\n",
        "In graph theory this is called finding connected components.\n",
        "\n",
        "Below, this will be implemented in Spark via the GraphFrames module, in Spark  iterating over dataframes, and also in Python.\n",
        "\n",
        "The mock data used below results in the following connected components.  \n",
        "  \n",
        "_```(1, 2, 3, 30)```_  \n",
        "_```(4, 5)```_  \n",
        "_```(6, 7, 8, 9, 23)```_  \n",
        "_```(10, 11, 12, 15, 17, 18, 20, 21)```_  \n",
        "<br>\n",
        "_Note - I haven't re-shaped each of the outputs below to a standard format in the interests of time.  This is relatively easy with some standard data munging._\n",
        "<br>\n",
        "# Set up\n",
        "\n",
        "Download Spark and install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-vzqUSrry5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1014f6a-1015-4b5f-9733-57337e92688e"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!rm -rf spark-3.0.2-bin-hadoop2.7.tgz*\n",
        "!pip -q install findspark pyspark graphframes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.3MB 72kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 61.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 45.0MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jbA6sUZr1qv"
      },
      "source": [
        "Set the environment variables so that Colab can find Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUMD0EHDr2O-"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "os.environ[\"HADOOP_HOME\"] = os.environ[\"SPARK_HOME\"]\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTSYzb29r4dz"
      },
      "source": [
        "Add PySpark to sys.path\n",
        "\n",
        "PySpark isn't on sys.path by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. [findspark](https://github.com/minrk/findspark) does the latter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxCizwptr996"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7itCha-sCU1"
      },
      "source": [
        "Create the Spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj2ZJUZQsC_a"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()  #.appName('test')\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu9hSKf77om1"
      },
      "source": [
        "Prior to using PySpark we need the required classes from the PySpark sql module and the GraphFrames module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMkJ3-3t7ryd"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window as W\n",
        "from pyspark.sql import Column as C\n",
        "from pyspark.sql import GroupedData as G\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.types import *\n",
        "from graphframes import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NvxwOs4tcL5"
      },
      "source": [
        "### Data\n",
        "\n",
        "Load mock data to a PySpark data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3Z2akXY8rQM"
      },
      "source": [
        "# In a list\n",
        "raw_matches_list = [\n",
        "    [1,2]\n",
        "    ,[1,30]\n",
        "    ,[2,3]\n",
        "    ,[2,3]\n",
        "    ,[4,5]\n",
        "    ,[6,7]\n",
        "    ,[7,8]\n",
        "    ,[8,9]\n",
        "    ,[10,12]\n",
        "    ,[10,15]\n",
        "    ,[10,17]\n",
        "    ,[11,15]\n",
        "    ,[15,20]\n",
        "    ,[17,18]\n",
        "    ,[20,21]\n",
        "    ,[23,8]\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABmSiySIrm_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1a9263-f36b-4871-e0f6-51c6d1963f22"
      },
      "source": [
        "# Data frame\n",
        "raw_matches = spark.createDataFrame(\n",
        "  raw_matches_list,\n",
        "  ['ecid_1', 'ecid_2', ] \n",
        ")\n",
        "raw_matches.show(20,truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------+\n",
            "|ecid_1|ecid_2|\n",
            "+------+------+\n",
            "|1     |2     |\n",
            "|1     |30    |\n",
            "|2     |3     |\n",
            "|2     |3     |\n",
            "|4     |5     |\n",
            "|6     |7     |\n",
            "|7     |8     |\n",
            "|8     |9     |\n",
            "|10    |12    |\n",
            "|10    |15    |\n",
            "|10    |17    |\n",
            "|11    |15    |\n",
            "|15    |20    |\n",
            "|17    |18    |\n",
            "|20    |21    |\n",
            "|23    |8     |\n",
            "+------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwjMpdjJ1WIg"
      },
      "source": [
        "Create vertices and edges data frames using the data above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL2nRQ9d1dD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53821af5-5e52-4dd6-8cf4-2f9f357e72b3"
      },
      "source": [
        "edges = raw_matches.withColumnRenamed('ecid_1','src').withColumnRenamed('ecid_2','dst').withColumn('strength', F.lit(1))\n",
        "\n",
        "edges.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+--------+\n",
            "|src|dst|strength|\n",
            "+---+---+--------+\n",
            "|1  |2  |1       |\n",
            "|1  |30 |1       |\n",
            "|2  |3  |1       |\n",
            "|2  |3  |1       |\n",
            "|4  |5  |1       |\n",
            "|6  |7  |1       |\n",
            "|7  |8  |1       |\n",
            "|8  |9  |1       |\n",
            "|10 |12 |1       |\n",
            "|10 |15 |1       |\n",
            "|10 |17 |1       |\n",
            "|11 |15 |1       |\n",
            "|15 |20 |1       |\n",
            "|17 |18 |1       |\n",
            "|20 |21 |1       |\n",
            "|23 |8  |1       |\n",
            "+---+---+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wImmT2TRtfP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16210ec5-2fb2-4aff-cfb0-ea1725d257d2"
      },
      "source": [
        "vertices = (raw_matches\n",
        "  .select('ecid_1')\n",
        "  .unionAll(raw_matches.select('ecid_2'))\n",
        "  .distinct()\n",
        "  .withColumnRenamed('ecid_1','id')\n",
        "  .withColumn('block_key', F.lit('block_key_01'))\n",
        ")\n",
        "\n",
        "vertices.show(truncate=False)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------+\n",
            "|id |block_key   |\n",
            "+---+------------+\n",
            "|7  |block_key_01|\n",
            "|6  |block_key_01|\n",
            "|9  |block_key_01|\n",
            "|17 |block_key_01|\n",
            "|5  |block_key_01|\n",
            "|1  |block_key_01|\n",
            "|10 |block_key_01|\n",
            "|3  |block_key_01|\n",
            "|12 |block_key_01|\n",
            "|8  |block_key_01|\n",
            "|11 |block_key_01|\n",
            "|2  |block_key_01|\n",
            "|4  |block_key_01|\n",
            "|18 |block_key_01|\n",
            "|21 |block_key_01|\n",
            "|15 |block_key_01|\n",
            "|30 |block_key_01|\n",
            "|23 |block_key_01|\n",
            "|20 |block_key_01|\n",
            "+---+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5lHz6bC3Azv"
      },
      "source": [
        "# Connected components using GraphFrames\n",
        "\n",
        "Create GraphFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEiYAbqj29Xp"
      },
      "source": [
        "g_ecid = GraphFrame(vertices, edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib--VE5f3HiC"
      },
      "source": [
        "Call connected components function.  Note that this requires setting a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrIiSdcQ3IDa"
      },
      "source": [
        "sc.setCheckpointDir(\"/tmp/graphframes_eg1_cc\")\n",
        "gcc = g_ecid.connectedComponents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ5kaXAe7fY4"
      },
      "source": [
        "Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ww9kb4B512o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a0bccc-1240-425d-b7e7-14b286f351cc"
      },
      "source": [
        "gcc.orderBy('component', 'id').show(20, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------+---------+\n",
            "|id |block_key   |component|\n",
            "+---+------------+---------+\n",
            "|1  |block_key_01|1        |\n",
            "|2  |block_key_01|1        |\n",
            "|3  |block_key_01|1        |\n",
            "|30 |block_key_01|1        |\n",
            "|4  |block_key_01|4        |\n",
            "|5  |block_key_01|4        |\n",
            "|6  |block_key_01|6        |\n",
            "|7  |block_key_01|6        |\n",
            "|8  |block_key_01|6        |\n",
            "|9  |block_key_01|6        |\n",
            "|23 |block_key_01|6        |\n",
            "|10 |block_key_01|10       |\n",
            "|11 |block_key_01|10       |\n",
            "|12 |block_key_01|10       |\n",
            "|15 |block_key_01|10       |\n",
            "|17 |block_key_01|10       |\n",
            "|18 |block_key_01|10       |\n",
            "|20 |block_key_01|10       |\n",
            "|21 |block_key_01|10       |\n",
            "+---+------------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aZGTzV66wZ_"
      },
      "source": [
        "# Connected components using Python\n",
        "\n",
        "The function below uses recursion to iterate through each tuple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SGQR1f064le"
      },
      "source": [
        "def connected_components(pairs):\n",
        "    # build a graph using the pairs\n",
        "    nodes = defaultdict(lambda: [])\n",
        "    for a, b in pairs:\n",
        "        if b is not None:\n",
        "            nodes[a].append((b, nodes[b]))\n",
        "            nodes[b].append((a, nodes[a]))\n",
        "        else:\n",
        "            nodes[a]  # empty list\n",
        "\n",
        "    # add all neighbors to the same group    \n",
        "    visited = set()\n",
        "    def _build_group(key, group):\n",
        "        if key in visited:\n",
        "            return\n",
        "        visited.add(key)\n",
        "        group.add(key)\n",
        "        for key, _ in nodes[key]:\n",
        "            _build_group(key, group)\n",
        "\n",
        "    groups = []\n",
        "    for key in nodes.keys():\n",
        "        if key in visited: continue\n",
        "        groups.append(set())\n",
        "        _build_group(key, groups[-1])\n",
        "\n",
        "    return groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Or6lqkK7DIN"
      },
      "source": [
        "Return connected components - this will be a list of sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjU2OeBF66zJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51e65d1-c23b-4dd8-dd64-ecfea9fcd038"
      },
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "cc_set = connected_components(raw_matches_list)\n",
        "cc_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{1, 2, 3, 30}, {4, 5}, {6, 7, 8, 9, 23}, {10, 11, 12, 15, 17, 18, 20, 21}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLP8Q0ka7NL9"
      },
      "source": [
        "Reshape to desired format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efZuMgsW7P_s"
      },
      "source": [
        "# Convert list of sets to list of lists \n",
        "cc_list = []\n",
        "for s in cc_set:\n",
        "  cc_list.append(list(s))\n",
        "\n",
        "\n",
        "# Convert to pandas data frame\n",
        "cc_list2pd = pd.DataFrame({'col1': cc_list})\n",
        "\n",
        "                                  \n",
        "cust_schema = StructType([\n",
        "  StructField('ecid_array', ArrayType(LongType()), True)\n",
        "])\n",
        "\n",
        "\n",
        "# Convert to spark data frame\n",
        "df3 = spark.createDataFrame(cc_list2pd,schema=cust_schema)\n",
        "\n",
        "\n",
        "# Reshape\n",
        "df4 = (df3\n",
        "  .withColumn('ecid_1',F.explode(F.col('ecid_array')))\n",
        "  .withColumn('match_key_2',F.sort_array(F.col('ecid_array')))\n",
        "  .withColumn('match_key_2', F.concat_ws('_',F.col('match_key_2')))\n",
        "  .withColumn('match_key_1', F.min('ecid_1').over(W.partitionBy('match_key_2')))\n",
        "  .orderBy('match_key_2','ecid_1')\n",
        "  .withColumnRenamed('ecid','ecid_1')\n",
        "  .select('ecid_1','match_key_1','match_key_2')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8WYq0yB7i8c"
      },
      "source": [
        "Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXbXFiqi7jdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4314ab-0b28-4b38-b15e-6d92f8df85ef"
      },
      "source": [
        "df4.orderBy('match_key_1','ecid_1').show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-----------+-----------------------+\n",
            "|ecid_1|match_key_1|match_key_2            |\n",
            "+------+-----------+-----------------------+\n",
            "|1     |1          |1_2_3_30               |\n",
            "|2     |1          |1_2_3_30               |\n",
            "|3     |1          |1_2_3_30               |\n",
            "|30    |1          |1_2_3_30               |\n",
            "|4     |4          |4_5                    |\n",
            "|5     |4          |4_5                    |\n",
            "|6     |6          |6_7_8_9_23             |\n",
            "|7     |6          |6_7_8_9_23             |\n",
            "|8     |6          |6_7_8_9_23             |\n",
            "|9     |6          |6_7_8_9_23             |\n",
            "|23    |6          |6_7_8_9_23             |\n",
            "|10    |10         |10_11_12_15_17_18_20_21|\n",
            "|11    |10         |10_11_12_15_17_18_20_21|\n",
            "|12    |10         |10_11_12_15_17_18_20_21|\n",
            "|15    |10         |10_11_12_15_17_18_20_21|\n",
            "|17    |10         |10_11_12_15_17_18_20_21|\n",
            "|18    |10         |10_11_12_15_17_18_20_21|\n",
            "|20    |10         |10_11_12_15_17_18_20_21|\n",
            "|21    |10         |10_11_12_15_17_18_20_21|\n",
            "+------+-----------+-----------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2n4343U6qlI"
      },
      "source": [
        "# Connected components using PySpark dataframes\n",
        "\n",
        "Ideally this should use checkpointing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H89alGoO_rbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4675546b-b29f-4f44-cba8-e811e613677d"
      },
      "source": [
        "# Convert test data to required array format\n",
        "t1 = (raw_matches\n",
        ".withColumn('match_key1', F.array(F.col('ecid_1'),F.col('ecid_2')))\n",
        ".select('match_key1')\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# All ecid's in an array to iterate over\n",
        "all_ecid = (t1\n",
        "  .withColumn('group', F.lit(1))\n",
        "  .groupBy('group')\n",
        "  .agg(F.array_distinct(F.flatten(F.collect_list('match_key1'))).alias('ecid_array'))\n",
        "  .withColumn('ecid_array', F.array_sort('ecid_array'))\n",
        "  .orderBy('ecid_array')\n",
        "  .drop('group')\n",
        "  .persist()\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Count of ecid's\n",
        "ecid_count = all_ecid.select(F.size('ecid_array')).collect()[0][0]\n",
        "\n",
        "\n",
        "# Loop\n",
        "t2 = t1\n",
        "for i in range(0,ecid_count):\n",
        "  print('Index: ',i)\n",
        "  \n",
        "  # Get distinct ecids in group\n",
        "  all_ecid_count = t2.withColumn('group_by',F.lit(1)).groupBy('group_by') \\\n",
        "  .agg(F.flatten(F.collect_list('match_key1')).alias('match_key1')) \\\n",
        "  .withColumn('all',F.explode('match_key1')) \\\n",
        "  .groupBy('all').count() \\\n",
        "  .orderBy('all')\n",
        "  \n",
        "  # Continue if there is only one ecid left in the grouped dataset\n",
        "  ecid_no = all_ecid_count.filter(F.col('all') == all_ecid.collect()[0][0][i]).select('count')\n",
        "  ecid_no = ecid_no.collect()[0][0]\n",
        "  if ecid_no == 1:\n",
        "    continue\n",
        "  \n",
        "  # Identify all arrays containing the current ecid selected by the loop and group and gather to one array\n",
        "  t3 = all_ecid.withColumn('ecid',F.col('ecid_array').getItem(i)).crossJoin(t2) \\\n",
        "  .withColumn('ecid', F.array('ecid')) \\\n",
        "  .withColumn('overlap',F.arrays_overlap('match_key1', 'ecid'))\\\n",
        "  .withColumn('group_by',F.when(F.col('overlap') == True, F.col('ecid')).otherwise(F.col('match_key1'))) \\\n",
        "  .groupBy('group_by') \\\n",
        "  .agg(F.flatten(F.collect_list('match_key1')).alias('match_key1')) \\\n",
        "  .withColumn('match_key1',F.array_distinct('match_key1')) \\\n",
        "  .withColumn('match_key1', F.array_sort('match_key1')) \\\n",
        "  .orderBy('match_key1') \\\n",
        "  .drop('group_by')\n",
        "  t2 = t3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index:  0\n",
            "Index:  1\n",
            "Index:  2\n",
            "Index:  3\n",
            "Index:  4\n",
            "Index:  5\n",
            "Index:  6\n",
            "Index:  7\n",
            "Index:  8\n",
            "Index:  9\n",
            "Index:  10\n",
            "Index:  11\n",
            "Index:  12\n",
            "Index:  13\n",
            "Index:  14\n",
            "Index:  15\n",
            "Index:  16\n",
            "Index:  17\n",
            "Index:  18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k-jKSV17_aq"
      },
      "source": [
        "Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJhcUlUn8E4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f2c0b7-4a71-42f3-e35b-568edd13e2ab"
      },
      "source": [
        "t2.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------+\n",
            "|match_key1                      |\n",
            "+--------------------------------+\n",
            "|[1, 2, 3, 30]                   |\n",
            "|[4, 5]                          |\n",
            "|[6, 7, 8, 9, 23]                |\n",
            "|[10, 11, 12, 15, 17, 18, 20, 21]|\n",
            "+--------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa_Dd0m-5v9j"
      },
      "source": [
        "## References\n",
        "\n",
        "[Checkpointing 1](https://enigma.com/blog/post/things-i-wish-id-known-about-spark)  \n",
        "[Checkpointing 2](https://dzone.com/articles/what-are-spark-checkpoints-on-dataframes)  \n",
        "[Checkpointing 3](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-checkpointing.html)  \n",
        "[Dataframe implementation (Scala)](https://blogs.oracle.com/ai-and-datascience/post/graph-computations-with-apache-spark)  \n",
        "[Dataframe implementation (PySpark)](https://towardsdatascience.com/connected-components-at-scale-in-pyspark-4a1c6423b9ed)  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsphAgTaMG6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0defc9-c4c8-415b-d546-db7c6129fa2e"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spark-3.0.2-bin-hadoop2.7\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}